{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Transform cleaned data into model-ready features: one-hot encoding, standardization, train/val/test split, and SMOTE for class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"../data\"\n",
    "INPUT_FILE = os.path.join(DATA_DIR, \"cleaned_loan_data.csv\")\n",
    "\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"train.csv\")\n",
    "VAL_FILE = os.path.join(DATA_DIR, \"val.csv\")\n",
    "TEST_FILE = os.path.join(DATA_DIR, \"test.csv\")\n",
    "\n",
    "TARGET_COL = \"status\"\n",
    "ID_COL = \"id\"\n",
    "\n",
    "NUMERIC_COLS = [\"term\", \"credit_score\", \"ltv\", \"dtir1\",\n",
    "                \"loan_amount\", \"income\", \"property_value\", \"year\"]\n",
    "\n",
    "CATEGORICAL_COLS = [\"loan_limit\", \"gender\", \"approv_in_adv\", \"loan_type\", \"loan_purpose\",\n",
    "                    \"credit_worthiness\", \"open_credit\", \"business_or_commercial\", \"neg_ammortization\",\n",
    "                    \"interest_only\", \"lump_sum_payment\", \"construction_type\", \"occupancy_type\",\n",
    "                    \"secured_by\", \"total_units\", \"credit_type\", \"co-applicant_credit_type\",\n",
    "                    \"age\", \"submission_of_application\", \"region\", \"security_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_data(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessor():\n",
    "    # Build ColumnTransformer: one-hot encode categorical, standardize numerical\n",
    "    numeric_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(\n",
    "        handle_unknown=\"ignore\",\n",
    "        sparse_output=False\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, NUMERIC_COLS),\n",
    "            (\"cat\", categorical_transformer, CATEGORICAL_COLS),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(X, y, random_state=42):\n",
    "    # Create 80/10/10 train/val/test split with stratification\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.20, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote(X_train, y_train, random_state=42):\n",
    "    # Apply SMOTE to training set only to address class imbalance\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: (148670, 29)\n",
      "Target distribution: {0: 112031, 1: 36639}\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = load_clean_data(INPUT_FILE)\n",
    "\n",
    "y = df[TARGET_COL]\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "\n",
    "if ID_COL in X.columns:\n",
    "    X = X.drop(columns=[ID_COL])\n",
    "\n",
    "print(f\"Data loaded: {X.shape}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate feature columns\n",
    "expected_cols = set(NUMERIC_COLS + CATEGORICAL_COLS)\n",
    "missing = expected_cols - set(X.columns)\n",
    "extra = set(X.columns) - expected_cols\n",
    "\n",
    "if missing:\n",
    "    raise ValueError(f\"Columns listed in NUMERIC/CATEGORICAL_COLS not in data: {missing}\")\n",
    "if extra:\n",
    "    print(f\"Warning: extra columns not in NUMERIC/CATEGORICAL_COLS: {extra}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 118936 samples\n",
      "Val: 14867 samples\n",
      "Test: 14867 samples\n"
     ]
    }
   ],
   "source": [
    "# Split into train/val/test\n",
    "X_train_raw, X_val_raw, X_test_raw, y_train, y_val, y_test = train_val_test_split(X, y)\n",
    "\n",
    "print(f\"Train: {X_train_raw.shape[0]} samples\")\n",
    "print(f\"Val: {X_val_raw.shape[0]} samples\")\n",
    "print(f\"Test: {X_test_raw.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after preprocessing: 67\n"
     ]
    }
   ],
   "source": [
    "# Build and fit preprocessor\n",
    "preprocessor = build_preprocessor()\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train_raw)\n",
    "X_val_processed = preprocessor.transform(X_val_raw)\n",
    "X_test_processed = preprocessor.transform(X_test_raw)\n",
    "\n",
    "print(f\"Features after preprocessing: {X_train_processed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover feature names after one-hot encoding\n",
    "ohe = preprocessor.named_transformers_[\"cat\"]\n",
    "cat_feature_names = list(ohe.get_feature_names_out(CATEGORICAL_COLS))\n",
    "num_feature_names = NUMERIC_COLS\n",
    "all_feature_names = num_feature_names + cat_feature_names\n",
    "\n",
    "# Convert to DataFrames\n",
    "X_train_df = pd.DataFrame(X_train_processed, columns=all_feature_names, index=X_train_raw.index)\n",
    "X_val_df = pd.DataFrame(X_val_processed, columns=all_feature_names, index=X_val_raw.index)\n",
    "X_test_df = pd.DataFrame(X_test_processed, columns=all_feature_names, index=X_test_raw.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set after SMOTE: 179250 samples\n",
      "Class distribution: {0: 89625, 1: 89625}\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE to training set only\n",
    "X_train_balanced, y_train_balanced = apply_smote(X_train_df, y_train)\n",
    "\n",
    "X_train_balanced_df = pd.DataFrame(X_train_balanced, columns=all_feature_names)\n",
    "\n",
    "print(f\"Training set after SMOTE: {X_train_balanced_df.shape[0]} samples\")\n",
    "print(f\"Class distribution: {pd.Series(y_train_balanced).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train set to ../data/train.csv (shape: (179250, 68))\n",
      "Saved validation set to ../data/val.csv (shape: (14867, 68))\n",
      "Saved test set to ../data/test.csv (shape: (14867, 68))\n"
     ]
    }
   ],
   "source": [
    "# Save train/val/test splits\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "train_out = X_train_balanced_df.copy()\n",
    "train_out[TARGET_COL] = y_train_balanced\n",
    "train_out.to_csv(TRAIN_FILE, index=False)\n",
    "\n",
    "val_out = X_val_df.copy()\n",
    "val_out[TARGET_COL] = y_val\n",
    "val_out.to_csv(VAL_FILE, index=False)\n",
    "\n",
    "test_out = X_test_df.copy()\n",
    "test_out[TARGET_COL] = y_test\n",
    "test_out.to_csv(TEST_FILE, index=False)\n",
    "\n",
    "print(f\"Saved train set to {TRAIN_FILE} (shape: {train_out.shape})\")\n",
    "print(f\"Saved validation set to {VAL_FILE} (shape: {val_out.shape})\")\n",
    "print(f\"Saved test set to {TEST_FILE} (shape: {test_out.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessor to ../models/preprocessor.pkl\n",
      "Saved feature names to ../models/feature_names.txt\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessor for future use (DiCE counterfactuals)\n",
    "MODELS_DIR = \"../models\"\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "preprocessor_path = os.path.join(MODELS_DIR, \"preprocessor.pkl\")\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f\"Saved preprocessor to {preprocessor_path}\")\n",
    "\n",
    "# Save feature names for reference\n",
    "feature_names_path = os.path.join(MODELS_DIR, \"feature_names.txt\")\n",
    "with open(feature_names_path, 'w') as f:\n",
    "    for fname in all_feature_names:\n",
    "        f.write(f\"{fname}\\n\")\n",
    "print(f\"Saved feature names to {feature_names_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
