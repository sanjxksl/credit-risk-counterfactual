{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron Model\n",
    "\n",
    "Train a 3-layer neural network using PyTorch with early stopping for loan default prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T16:37:29.013763Z",
     "iopub.status.busy": "2025-12-05T16:37:29.013355Z",
     "iopub.status.idle": "2025-12-05T16:37:30.595783Z",
     "shell.execute_reply": "2025-12-05T16:37:30.595504Z"
    }
   },
   "outputs": [],
   "source": "import os\nimport json\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.metrics import (\n    roc_auc_score,\n    average_precision_score,\n    brier_score_loss,\n    classification_report,\n    confusion_matrix\n)\n\n# Configuration\nDATA_DIR = \"../data\"\nMODELS_DIR = \"../models\"\nRESULTS_DIR = \"../results\"\n\nTRAIN_FILE = os.path.join(DATA_DIR, \"train.csv\")\nVAL_FILE = os.path.join(DATA_DIR, \"val.csv\")\nTEST_FILE = os.path.join(DATA_DIR, \"test.csv\")\n\nMODEL_FILE = os.path.join(MODELS_DIR, \"mlp_model.pth\")\nPREDICTIONS_FILE = os.path.join(RESULTS_DIR, \"mlp_predictions.csv\")\nMETRICS_FILE = os.path.join(RESULTS_DIR, \"mlp_metrics.json\")\n\nTARGET_COL = \"status\"\nRANDOM_STATE = 42\n\nBATCH_SIZE = 64\nMAX_EPOCHS = 200\nLEARNING_RATE = 0.0008\nPATIENCE = 20\nWEIGHT_DECAY = 1e-5\nFOCAL_ALPHA = 0.3\nFOCAL_GAMMA = 2.5\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntorch.manual_seed(RANDOM_STATE)\nnp.random.seed(RANDOM_STATE)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(RANDOM_STATE)\n\nos.makedirs(MODELS_DIR, exist_ok=True)\nos.makedirs(RESULTS_DIR, exist_ok=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T16:37:30.599193Z",
     "iopub.status.busy": "2025-12-05T16:37:30.599010Z",
     "iopub.status.idle": "2025-12-05T16:37:30.604654Z",
     "shell.execute_reply": "2025-12-05T16:37:30.604450Z"
    }
   },
   "outputs": [],
   "source": "# Define Focal Loss for imbalanced classification\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.0):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        \n    def forward(self, inputs, targets):\n        bce_loss = nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-bce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n        return focal_loss.mean()\n\n# Define Residual Block\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, dropout=0.3):\n        super(ResidualBlock, self).__init__()\n        self.block = nn.Sequential(\n            nn.Linear(dim, dim),\n            nn.BatchNorm1d(dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim, dim),\n            nn.BatchNorm1d(dim)\n        )\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        residual = x\n        out = self.block(x)\n        out += residual\n        out = self.relu(out)\n        out = self.dropout(out)\n        return out\n\n# Define Advanced Deep MLP\nclass SimpleMLP(nn.Module):\n    def __init__(self, input_dim):\n        super(SimpleMLP, self).__init__()\n        \n        # Larger initial projection\n        self.input_layer = nn.Sequential(\n            nn.Linear(input_dim, 768),\n            nn.BatchNorm1d(768),\n            nn.ReLU(),\n            nn.Dropout(0.5)\n        )\n        \n        # More residual blocks at high dimension\n        self.res_block1 = ResidualBlock(768, dropout=0.4)\n        self.res_block2 = ResidualBlock(768, dropout=0.4)\n        self.res_block3 = ResidualBlock(768, dropout=0.4)\n        \n        # Gradual downsampling\n        self.down1 = nn.Sequential(\n            nn.Linear(768, 384),\n            nn.BatchNorm1d(384),\n            nn.ReLU(),\n            nn.Dropout(0.4)\n        )\n        \n        self.res_block4 = ResidualBlock(384, dropout=0.35)\n        self.res_block5 = ResidualBlock(384, dropout=0.35)\n        \n        self.down2 = nn.Sequential(\n            nn.Linear(384, 192),\n            nn.BatchNorm1d(192),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        \n        self.res_block6 = ResidualBlock(192, dropout=0.3)\n        \n        self.down3 = nn.Sequential(\n            nn.Linear(192, 96),\n            nn.BatchNorm1d(96),\n            nn.ReLU(),\n            nn.Dropout(0.25)\n        )\n        \n        self.res_block7 = ResidualBlock(96, dropout=0.25)\n        \n        # Final layer\n        self.output_layer = nn.Sequential(\n            nn.Linear(96, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        x = self.input_layer(x)\n        x = self.res_block1(x)\n        x = self.res_block2(x)\n        x = self.res_block3(x)\n        x = self.down1(x)\n        x = self.res_block4(x)\n        x = self.res_block5(x)\n        x = self.down2(x)\n        x = self.res_block6(x)\n        x = self.down3(x)\n        x = self.res_block7(x)\n        x = self.output_layer(x)\n        return x"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T16:37:30.605870Z",
     "iopub.status.busy": "2025-12-05T16:37:30.605768Z",
     "iopub.status.idle": "2025-12-05T16:37:31.181670Z",
     "shell.execute_reply": "2025-12-05T16:37:31.181385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (179250, 67)\n",
      "Validation set: (14867, 67)\n",
      "Test set: (14867, 67)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(TRAIN_FILE)\n",
    "val_df = pd.read_csv(VAL_FILE)\n",
    "test_df = pd.read_csv(TEST_FILE)\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET_COL]).values\n",
    "y_train = train_df[TARGET_COL].values\n",
    "\n",
    "X_val = val_df.drop(columns=[TARGET_COL]).values\n",
    "y_val = val_df[TARGET_COL].values\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET_COL]).values\n",
    "y_test = test_df[TARGET_COL].values\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T16:37:31.204379Z",
     "iopub.status.busy": "2025-12-05T16:37:31.204241Z",
     "iopub.status.idle": "2025-12-05T16:37:31.219818Z",
     "shell.execute_reply": "2025-12-05T16:37:31.219514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.FloatTensor(y_val).unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T16:37:31.221807Z",
     "iopub.status.busy": "2025-12-05T16:37:31.221682Z",
     "iopub.status.idle": "2025-12-05T16:37:31.613864Z",
     "shell.execute_reply": "2025-12-05T16:37:31.613606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with 1430145 parameters\n",
      "Training on device: cpu\n",
      "Using Focal Loss with alpha=0.25, gamma=2.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "input_dim = X_train.shape[1]\n",
    "model = SimpleMLP(input_dim).to(DEVICE)\n",
    "criterion = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "print(f\"Training on device: {DEVICE}\")\n",
    "print(f\"Using Focal Loss with alpha={FOCAL_ALPHA}, gamma={FOCAL_GAMMA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T16:37:31.615346Z",
     "iopub.status.busy": "2025-12-05T16:37:31.615195Z",
     "iopub.status.idle": "2025-12-05T16:37:31.618153Z",
     "shell.execute_reply": "2025-12-05T16:37:31.617927Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training functions\n",
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T16:37:31.619720Z",
     "iopub.status.busy": "2025-12-05T16:37:31.619627Z",
     "iopub.status.idle": "2025-12-05T16:42:50.419323Z",
     "shell.execute_reply": "2025-12-05T16:42:50.418683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150 - Train Loss: 0.0300, Val Loss: 0.0248, Val AUC: 0.8684, LR: 0.000976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/150 - Train Loss: 0.0271, Val Loss: 0.0236, Val AUC: 0.8775, LR: 0.000905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/150 - Train Loss: 0.0264, Val Loss: 0.0235, Val AUC: 0.8793, LR: 0.000794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/150 - Train Loss: 0.0259, Val Loss: 0.0234, Val AUC: 0.8808, LR: 0.000655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/150 - Train Loss: 0.0255, Val Loss: 0.0225, Val AUC: 0.8838, LR: 0.000501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/150 - Train Loss: 0.0251, Val Loss: 0.0226, Val AUC: 0.8855, LR: 0.000346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/150 - Train Loss: 0.0247, Val Loss: 0.0221, Val AUC: 0.8864, LR: 0.000207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/150 - Train Loss: 0.0245, Val Loss: 0.0227, Val AUC: 0.8858, LR: 0.000096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/150 - Train Loss: 0.0243, Val Loss: 0.0218, Val AUC: 0.8862, LR: 0.000025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/150 - Train Loss: 0.0241, Val Loss: 0.0219, Val AUC: 0.8867, LR: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/150 - Train Loss: 0.0249, Val Loss: 0.0223, Val AUC: 0.8838, LR: 0.000994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/150 - Train Loss: 0.0247, Val Loss: 0.0227, Val AUC: 0.8767, LR: 0.000976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/150 - Train Loss: 0.0245, Val Loss: 0.0213, Val AUC: 0.8848, LR: 0.000946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/150 - Train Loss: 0.0243, Val Loss: 0.0224, Val AUC: 0.8832, LR: 0.000905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/150 - Train Loss: 0.0240, Val Loss: 0.0217, Val AUC: 0.8835, LR: 0.000854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/150 - Train Loss: 0.0238, Val Loss: 0.0223, Val AUC: 0.8842, LR: 0.000794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/150 - Train Loss: 0.0236, Val Loss: 0.0215, Val AUC: 0.8828, LR: 0.000727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/150 - Train Loss: 0.0233, Val Loss: 0.0211, Val AUC: 0.8847, LR: 0.000655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/150 - Train Loss: 0.0231, Val Loss: 0.0212, Val AUC: 0.8844, LR: 0.000579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/150 - Train Loss: 0.0229, Val Loss: 0.0214, Val AUC: 0.8836, LR: 0.000501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/150 - Train Loss: 0.0227, Val Loss: 0.0255, Val AUC: 0.8723, LR: 0.000422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/150 - Train Loss: 0.0225, Val Loss: 0.0207, Val AUC: 0.8835, LR: 0.000346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/150 - Train Loss: 0.0223, Val Loss: 0.0204, Val AUC: 0.8847, LR: 0.000274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/150 - Train Loss: 0.0222, Val Loss: 0.0204, Val AUC: 0.8843, LR: 0.000207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/150 - Train Loss: 0.0220, Val Loss: 0.0202, Val AUC: 0.8854, LR: 0.000147\n",
      "Early stopping triggered after 25 epochs\n",
      "Best validation AUC: 0.8867\n",
      "Best validation loss: 0.0219\n"
     ]
    }
   ],
   "source": [
    "# Train with early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "best_val_auc = 0.0\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss = validate(model, val_loader, criterion)\n",
    "    \n",
    "    # Calculate validation AUC during training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_proba = model(torch.FloatTensor(X_val).to(DEVICE)).cpu().numpy().flatten()\n",
    "        val_auc = roc_auc_score(y_val, val_proba)\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{MAX_EPOCHS} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}, LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Save best model based on AUC (more relevant than loss)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f\"Best validation AUC: {best_val_auc:.4f}\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T16:42:50.423449Z",
     "iopub.status.busy": "2025-12-05T16:42:50.423276Z",
     "iopub.status.idle": "2025-12-05T16:42:50.546012Z",
     "shell.execute_reply": "2025-12-05T16:42:50.545533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Performance:\n",
      "AUC-ROC: 0.8854\n",
      "AUC-PR: 0.8322\n",
      "Brier Score: 0.1282\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93     11203\n",
      "           1       0.86      0.66      0.75      3664\n",
      "\n",
      "    accuracy                           0.89     14867\n",
      "   macro avg       0.88      0.81      0.84     14867\n",
      "weighted avg       0.89      0.89      0.88     14867\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10825   378]\n",
      " [ 1262  2402]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "model.eval()\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_val_proba = model(X_val_tensor).cpu().numpy().flatten()\n",
    "\n",
    "y_val_pred = (y_val_proba >= 0.5).astype(int)\n",
    "\n",
    "val_auc_roc = roc_auc_score(y_val, y_val_proba)\n",
    "val_auc_pr = average_precision_score(y_val, y_val_proba)\n",
    "val_brier = brier_score_loss(y_val, y_val_proba)\n",
    "\n",
    "print(\"Validation Set Performance:\")\n",
    "print(f\"AUC-ROC: {val_auc_roc:.4f}\")\n",
    "print(f\"AUC-PR: {val_auc_pr:.4f}\")\n",
    "print(f\"Brier Score: {val_brier:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T16:42:50.548315Z",
     "iopub.status.busy": "2025-12-05T16:42:50.548153Z",
     "iopub.status.idle": "2025-12-05T16:42:50.647900Z",
     "shell.execute_reply": "2025-12-05T16:42:50.647396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance:\n",
      "AUC-ROC: 0.8962\n",
      "AUC-PR: 0.8438\n",
      "Brier Score: 0.1262\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_test_proba = model(X_test_tensor).cpu().numpy().flatten()\n",
    "\n",
    "y_test_pred = (y_test_proba >= 0.5).astype(int)\n",
    "\n",
    "test_auc_roc = roc_auc_score(y_test, y_test_proba)\n",
    "test_auc_pr = average_precision_score(y_test, y_test_proba)\n",
    "test_brier = brier_score_loss(y_test, y_test_proba)\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"AUC-ROC: {test_auc_roc:.4f}\")\n",
    "print(f\"AUC-PR: {test_auc_pr:.4f}\")\n",
    "print(f\"Brier Score: {test_brier:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T16:42:50.650198Z",
     "iopub.status.busy": "2025-12-05T16:42:50.650021Z",
     "iopub.status.idle": "2025-12-05T16:42:50.698295Z",
     "shell.execute_reply": "2025-12-05T16:42:50.697880Z"
    }
   },
   "outputs": [],
   "source": "# Save model\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'input_dim': input_dim\n}, MODEL_FILE)\nprint(f\"Model saved to {MODEL_FILE}\")\n\n# Save predictions\npredictions_df = pd.DataFrame({\n    'true_label': y_val,\n    'predicted_probability': y_val_proba,\n    'predicted_label': y_val_pred,\n    'dataset': 'validation'\n})\npredictions_df.to_csv(PREDICTIONS_FILE, index=False)\nprint(f\"Predictions saved to {PREDICTIONS_FILE}\")\n\n# Save metrics\nall_metrics = {\n    'architecture': 'DeepResNet: Input → 768 (3x ResBlocks) → 384 (2x ResBlocks) → 192 (ResBlock) → 96 (ResBlock) → 1',\n    'features': 'Deep residual connections, Batch Normalization, Focal Loss, Gradient Clipping',\n    'total_residual_blocks': 7,\n    'dropout': 'Progressive: 0.5 → 0.4 → 0.35 → 0.3 → 0.25',\n    'batch_size': BATCH_SIZE,\n    'learning_rate': LEARNING_RATE,\n    'weight_decay': WEIGHT_DECAY,\n    'optimizer': 'AdamW',\n    'scheduler': 'CosineAnnealingWarmRestarts',\n    'loss_function': f'FocalLoss(alpha={FOCAL_ALPHA}, gamma={FOCAL_GAMMA})',\n    'max_epochs': MAX_EPOCHS,\n    'patience': PATIENCE,\n    'gradient_clipping': 1.0,\n    'validation_metrics': {\n        'auc_roc': float(val_auc_roc),\n        'auc_pr': float(val_auc_pr),\n        'brier_score': float(val_brier),\n        'dataset': 'Validation'\n    },\n    'test_metrics': {\n        'auc_roc': float(test_auc_roc),\n        'auc_pr': float(test_auc_pr),\n        'brier_score': float(test_brier),\n        'dataset': 'Test'\n    }\n}\n\nwith open(METRICS_FILE, 'w') as f:\n    json.dump(all_metrics, f, indent=4)\nprint(f\"Metrics saved to {METRICS_FILE}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}