{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble MLP Models\n",
    "\n",
    "Train multiple MLP models with different initializations and ensemble their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"../data\"\n",
    "MODELS_DIR = \"../models\"\n",
    "RESULTS_DIR = \"../results\"\n",
    "\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"train.csv\")\n",
    "VAL_FILE = os.path.join(DATA_DIR, \"val.csv\")\n",
    "TEST_FILE = os.path.join(DATA_DIR, \"test.csv\")\n",
    "\n",
    "MODEL_FILE = os.path.join(MODELS_DIR, \"mlp_model.pth\")\n",
    "PREDICTIONS_FILE = os.path.join(RESULTS_DIR, \"mlp_predictions.csv\")\n",
    "METRICS_FILE = os.path.join(RESULTS_DIR, \"mlp_metrics.json\")\n",
    "\n",
    "TARGET_COL = \"status\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPOCHS = 200\n",
    "LEARNING_RATE = 0.0005\n",
    "PATIENCE = 20\n",
    "WEIGHT_DECAY = 1e-5\n",
    "FOCAL_ALPHA = 0.3\n",
    "FOCAL_GAMMA = 2.5\n",
    "NUM_MODELS = 5\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Define Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "# Define Deep Residual MLP\n",
    "class DeepResidualMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, dropout=0.5):\n",
    "        super(DeepResidualMLP, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualBlock(hidden_dim, dropout=dropout * 0.8),\n",
    "            ResidualBlock(hidden_dim, dropout=dropout * 0.8),\n",
    "            ResidualBlock(hidden_dim, dropout=dropout * 0.8)\n",
    "        ])\n",
    "        \n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.8)\n",
    "        )\n",
    "        \n",
    "        self.res_blocks2 = nn.ModuleList([\n",
    "            ResidualBlock(hidden_dim // 2, dropout=dropout * 0.6)\n",
    "        ])\n",
    "        \n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.BatchNorm1d(hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.6)\n",
    "        )\n",
    "        \n",
    "        self.res_blocks3 = nn.ModuleList([\n",
    "            ResidualBlock(hidden_dim // 4, dropout=dropout * 0.4)\n",
    "        ])\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "        x = self.down1(x)\n",
    "        for block in self.res_blocks2:\n",
    "            x = block(x)\n",
    "        x = self.down2(x)\n",
    "        for block in self.res_blocks3:\n",
    "            x = block(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(TRAIN_FILE)\n",
    "val_df = pd.read_csv(VAL_FILE)\n",
    "test_df = pd.read_csv(TEST_FILE)\n",
    "\n",
    "X_train = train_df.drop(columns=[TARGET_COL]).values\n",
    "y_train = train_df[TARGET_COL].values\n",
    "\n",
    "X_val = val_df.drop(columns=[TARGET_COL]).values\n",
    "y_val = val_df[TARGET_COL].values\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET_COL]).values\n",
    "y_test = test_df[TARGET_COL].values\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "def get_predictions(model, X):\n",
    "    model.eval()\n",
    "    X_tensor = torch.FloatTensor(X).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_tensor).cpu().numpy().flatten()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble of models\n",
    "models = []\n",
    "val_predictions = []\n",
    "test_predictions = []\n",
    "\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Model {model_idx + 1}/{NUM_MODELS}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Set different seed for each model\n",
    "    seed = 42 + model_idx\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    # Create dataloaders with different shuffle\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.FloatTensor(y_val).unsqueeze(1)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = DeepResidualMLP(input_dim, hidden_dim=512, dropout=0.5).to(DEVICE)\n",
    "    criterion = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=15, T_mult=2, eta_min=1e-7)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Train\n",
    "    best_val_auc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss = validate(model, val_loader, criterion)\n",
    "        \n",
    "        # Calculate validation AUC\n",
    "        val_proba = get_predictions(model, X_val)\n",
    "        val_auc = roc_auc_score(y_val, val_proba)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{MAX_EPOCHS} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\nBest validation AUC for model {model_idx + 1}: {best_val_auc:.4f}\")\n",
    "    \n",
    "    # Store model and predictions\n",
    "    models.append(model)\n",
    "    val_predictions.append(get_predictions(model, X_val))\n",
    "    test_predictions.append(get_predictions(model, X_test))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All models trained!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble predictions (average)\n",
    "val_ensemble_proba = np.mean(val_predictions, axis=0)\n",
    "test_ensemble_proba = np.mean(test_predictions, axis=0)\n",
    "\n",
    "val_ensemble_pred = (val_ensemble_proba >= 0.5).astype(int)\n",
    "test_ensemble_pred = (test_ensemble_proba >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate ensemble on validation set\n",
    "val_auc_roc = roc_auc_score(y_val, val_ensemble_proba)\n",
    "val_auc_pr = average_precision_score(y_val, val_ensemble_proba)\n",
    "val_brier = brier_score_loss(y_val, val_ensemble_proba)\n",
    "\n",
    "print(\"Ensemble Validation Performance:\")\n",
    "print(f\"AUC-ROC: {val_auc_roc:.4f}\")\n",
    "print(f\"AUC-PR: {val_auc_pr:.4f}\")\n",
    "print(f\"Brier Score: {val_brier:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, val_ensemble_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_val, val_ensemble_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ensemble on test set\n",
    "test_auc_roc = roc_auc_score(y_test, test_ensemble_proba)\n",
    "test_auc_pr = average_precision_score(y_test, test_ensemble_proba)\n",
    "test_brier = brier_score_loss(y_test, test_ensemble_proba)\n",
    "\n",
    "print(\"Ensemble Test Performance:\")\n",
    "print(f\"AUC-ROC: {test_auc_roc:.4f}\")\n",
    "print(f\"AUC-PR: {test_auc_pr:.4f}\")\n",
    "print(f\"Brier Score: {test_brier:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, test_ensemble_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, test_ensemble_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model from ensemble\n",
    "torch.save({\n",
    "    'model_state_dict': models[0].state_dict(),\n",
    "    'input_dim': input_dim\n",
    "}, MODEL_FILE)\n",
    "print(f\"Best model saved to {MODEL_FILE}\")\n",
    "\n",
    "# Save ensemble predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'true_label': y_val,\n",
    "    'predicted_probability': val_ensemble_proba,\n",
    "    'predicted_label': val_ensemble_pred,\n",
    "    'dataset': 'validation'\n",
    "})\n",
    "predictions_df.to_csv(PREDICTIONS_FILE, index=False)\n",
    "print(f\"Ensemble predictions saved to {PREDICTIONS_FILE}\")\n",
    "\n",
    "# Save metrics\n",
    "all_metrics = {\n",
    "    'architecture': f'Ensemble of {NUM_MODELS} DeepResidualMLP models',\n",
    "    'base_architecture': 'Input → 512 (3x ResBlocks) → 256 (ResBlock) → 128 (ResBlock) → 1',\n",
    "    'features': 'Deep residual connections, BatchNorm, Focal Loss, Model averaging',\n",
    "    'num_models': NUM_MODELS,\n",
    "    'dropout': '0.5 → 0.4 → 0.3 → 0.2',\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "    'optimizer': 'AdamW',\n",
    "    'scheduler': 'CosineAnnealingWarmRestarts',\n",
    "    'loss_function': f'FocalLoss(alpha={FOCAL_ALPHA}, gamma={FOCAL_GAMMA})',\n",
    "    'max_epochs': MAX_EPOCHS,\n",
    "    'patience': PATIENCE,\n",
    "    'gradient_clipping': 0.5,\n",
    "    'validation_metrics': {\n",
    "        'auc_roc': float(val_auc_roc),\n",
    "        'auc_pr': float(val_auc_pr),\n",
    "        'brier_score': float(val_brier),\n",
    "        'dataset': 'Validation'\n",
    "    },\n",
    "    'test_metrics': {\n",
    "        'auc_roc': float(test_auc_roc),\n",
    "        'auc_pr': float(test_auc_pr),\n",
    "        'brier_score': float(test_brier),\n",
    "        'dataset': 'Test'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(METRICS_FILE, 'w') as f:\n",
    "    json.dump(all_metrics, f, indent=4)\n",
    "print(f\"Metrics saved to {METRICS_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
